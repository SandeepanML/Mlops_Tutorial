{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bc6ec1-da4e-47e1-90df-e5b927cb8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6148662d-22fe-4cff-b1e2-e205647461a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\write\\\\Desktop\\\\LSTM Projects\\\\dataset\\\\Electricity+Consumption.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a9133a-4cbd-4c1c-b872-2eeb87690290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146dbd5f-7c82-4b50-9a2a-9b1ba1cb9b53",
   "metadata": {},
   "source": [
    "#### Train and Test conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca90547-1bed-4b09-b233-d3b00bfc68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_data = df.iloc[:8712, 1:3].values\n",
    "test_data = df.iloc[8712:, 1:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c73f82-c8b8-4925-9ea4-2e673aa3fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_y = df.iloc[:8712, 3].values\n",
    "test_y = df.iloc[8712:, 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74e37ed-caaa-4df0-bea6-14a719452965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e8b3a8-7a9f-4ce0-989c-c2f822c1102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_data_scaled = sc.fit_transform(trained_data)\n",
    "test_data_scaled = sc.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df626ff7-8e76-4a8a-995e-71089b4ce0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_data_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f37af-40d0-45c8-97bb-649b2f751250",
   "metadata": {},
   "source": [
    "#### Create the Window Sized Dataset for Time Series Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60fd7a3-7334-4b6b-8a19-4f7e37d41e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_Size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31279fc2-ace1-4e37-8884-07b0a6dae8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars = []\n",
    "target_vars = []\n",
    "\n",
    "for i in range(window_Size, trained_data_scaled.shape[0]):\n",
    "    train_vars.append(trained_data_scaled[i - window_Size:i, 0:3])\n",
    "    target_vars.append(trained_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20af06de-9583-4a7b-810f-13b94db24ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(train_vars), np.array(target_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e95a8b-3d3a-4610-bee6-0ca73216bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshape = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c361404-0263-49dd-b451-058d36b7e179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8688, 24, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f66f0d2e-b6fb-4426-8de0-ba0d8c617dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8688, 24, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "856dfc9f-dc47-4ac1-9a10-62cfef9c35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "239a6b06-a75f-40ac-89fb-bc70bd55583c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8688,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59997ffc-bfee-4ff6-9a72-a45f8a071483",
   "metadata": {},
   "source": [
    "### Build the Multivariate LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46f1a466-f730-4fa0-b7f0-4d49652dfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e616aca3-3141-4400-9861-cb6fea6c0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLSTM = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "485c0e20-96f2-4420-b772-b74a89c135f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiLSTM.add(LSTM(units = 60, return_sequences = True, input_shape = (window_Size, X_train_reshape.shape[2])))\n",
    "multiLSTM.add(Dropout(0.1))\n",
    "multiLSTM.add(LSTM(units = 80, return_sequences = True))\n",
    "multiLSTM.add(Dropout(0.1))\n",
    "multiLSTM.add(LSTM(units = 60, return_sequences = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ea8c4-9f5f-468c-800e-80699f9ed450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc6349-b7da-416e-979e-e4bb27443b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40793fac-ec04-4416-a2f8-05c78476be31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73530f62-9e99-4c80-835d-e2574b53d026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07844b55-b772-46ab-b13c-c323dbf0d7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b4d56-3416-4120-a67b-c0b3ef6bd14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095237e-429c-47d8-b8b9-73668e92e5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bb378-02d4-456e-a323-9628884b96b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37186993-765a-4bb9-af17-7afd2ca959bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6b21c-f9f0-4d5e-b920-3190b36de370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7397f7-ba56-401e-bd7d-b5d36079486e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0e5da39-894b-4e1b-a213-516371bcc95c",
   "metadata": {},
   "source": [
    "## Interview Perspective : Sample for Understanding Shapes in LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057e060-d12e-40ac-8b9d-19e7a9386673",
   "metadata": {},
   "source": [
    "Certainly! Let's go through an example where we build a model with 5 LSTM layers and 2 Dense layers in between, using a window size of 10 and a batch size of 32.\n",
    "\n",
    "### Model Architecture Overview:\n",
    "- **Input shape:** `(batch_size=32, timesteps=10, features=1)` â†’ This represents a sliding window over time series data where each window has 10 timesteps and 1 feature.\n",
    "- **LSTM Layers:** 3 LSTM layers before the Dense layers and 2 LSTM layers after.\n",
    "- **Dense Layers:** 2 Dense layers inserted in the middle.\n",
    "- **Final Dense Layer:** Output layer to predict 1 value.\n",
    "\n",
    "### Example Code:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM Layer (input_shape=(timesteps=10, features=1))\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(10, 1)))\n",
    "# Shape: (batch_size, timesteps=10, features=64)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second LSTM Layer\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "# Shape: (batch_size, timesteps=10, features=64)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third LSTM Layer\n",
    "model.add(LSTM(units=64, return_sequences=False))  # No more sequences\n",
    "# Shape: (batch_size, features=64)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# First Dense Layer\n",
    "model.add(Dense(units=32))  # Reduces dimensionality\n",
    "# Shape: (batch_size, features=32)\n",
    "\n",
    "# Second Dense Layer\n",
    "model.add(Dense(units=16))  # Further reduces dimensionality\n",
    "# Shape: (batch_size, features=16)\n",
    "\n",
    "# Fourth LSTM Layer (returning sequences again)\n",
    "model.add(Reshape((1, 16)))  # Reshape to (batch_size, 1 timestep, 16 features) for LSTM compatibility\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "# Shape: (batch_size, timesteps=1, features=64)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fifth LSTM Layer (final LSTM layer)\n",
    "model.add(LSTM(units=64))\n",
    "# Shape: (batch_size, features=64)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Final Output Dense Layer\n",
    "model.add(Dense(units=1))  # Predicting a single output value\n",
    "# Shape: (batch_size, features=1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### Explanation of Shapes at Each Layer:\n",
    "\n",
    "1. **Input Shape: (batch_size=32, timesteps=10, features=1)**\n",
    "   - **Explanation:** The model is processing data in batches of 32 samples, with each sample being a window of 10 timesteps and 1 feature per timestep.\n",
    "\n",
    "2. **First LSTM Layer (64 units, return_sequences=True):**\n",
    "   - **Output Shape:** `(batch_size=32, timesteps=10, features=64)`\n",
    "   - **Explanation:** The LSTM layer processes each of the 10 timesteps and outputs a 64-dimensional vector for each timestep.\n",
    "\n",
    "3. **Second LSTM Layer (64 units, return_sequences=True):**\n",
    "   - **Output Shape:** `(batch_size=32, timesteps=10, features=64)`\n",
    "   - **Explanation:** Similar to the first LSTM, this layer outputs a 64-dimensional vector for each timestep.\n",
    "\n",
    "4. **Third LSTM Layer (64 units, return_sequences=False):**\n",
    "   - **Output Shape:** `(batch_size=32, features=64)`\n",
    "   - **Explanation:** Since `return_sequences=False`, this layer returns only the final output of the LSTM, which is a single 64-dimensional vector for each sample (ignores the timesteps).\n",
    "\n",
    "5. **First Dense Layer (32 units):**\n",
    "   - **Output Shape:** `(batch_size=32, features=32)`\n",
    "   - **Explanation:** The `Dense` layer reduces the dimensionality from 64 features to 32 features.\n",
    "\n",
    "6. **Second Dense Layer (16 units):**\n",
    "   - **Output Shape:** `(batch_size=32, features=16)`\n",
    "   - **Explanation:** Further reduces the dimensionality from 32 features to 16 features.\n",
    "\n",
    "7. **Reshape Layer:**\n",
    "   - **Output Shape:** `(batch_size=32, timesteps=1, features=16)`\n",
    "   - **Explanation:** We reshape the output to prepare it for further LSTM layers. It converts the flat feature vector of 16 into a timestep (1 timestep with 16 features).\n",
    "\n",
    "8. **Fourth LSTM Layer (64 units, return_sequences=True):**\n",
    "   - **Output Shape:** `(batch_size=32, timesteps=1, features=64)`\n",
    "   - **Explanation:** The LSTM layer processes the 1 timestep and outputs a 64-dimensional vector.\n",
    "\n",
    "9. **Fifth LSTM Layer (64 units, return_sequences=False):**\n",
    "   - **Output Shape:** `(batch_size=32, features=64)`\n",
    "   - **Explanation:** Since `return_sequences=False`, the final output is a 64-dimensional vector per sample.\n",
    "\n",
    "10. **Final Dense Layer (1 unit):**\n",
    "    - **Output Shape:** `(batch_size=32, features=1)`\n",
    "    - **Explanation:** This layer reduces the output to a single value per sample, suitable for regression tasks (like predicting a single value).\n",
    "\n",
    "### Key Points:\n",
    "- The `return_sequences=True` in LSTM layers ensures that each LSTM outputs a sequence for every timestep. This is needed for stacking LSTM layers.\n",
    "- The two `Dense` layers in the middle reduce the feature dimensionality.\n",
    "- After the `Dense` layers, we reshape the output to make it compatible with further LSTM layers.\n",
    "- The final `Dense` layer predicts a single value.\n",
    "\n",
    "This example shows how you can use multiple LSTM layers with Dense layers in between while preserving the sequential nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573408fd-1d51-4f76-b364-d4a99f4e663b",
   "metadata": {},
   "source": [
    "The `return_sequences` argument in an LSTM layer determines the format of the output from the LSTM. It controls whether the LSTM layer should return the entire sequence of outputs (one for each timestep) or just the output from the last timestep. Let's break this down:\n",
    "\n",
    "### **1. `return_sequences=False` (default behavior):**\n",
    "- **Output:** The LSTM will return only the output of the last timestep.\n",
    "- **Shape:** `(batch_size, units)` where `units` is the number of LSTM units (neurons).\n",
    "\n",
    "**When to use:**\n",
    "- You typically use `return_sequences=False` when the next layer only needs the final state of the sequence, such as when making a prediction at the end of the sequence (e.g., classification of a sequence, or predicting the next value based on all previous timesteps).\n",
    "\n",
    "**Example Shape:**\n",
    "- Input shape: `(batch_size, timesteps, features)` (e.g., `(32, 10, 1)` for 10 timesteps with 1 feature each).\n",
    "- LSTM output shape: `(batch_size, units)` (e.g., `(32, 60)` if there are 60 LSTM units).\n",
    "\n",
    "### **2. `return_sequences=True`:**\n",
    "- **Output:** The LSTM will return an output for every timestep in the input sequence, not just the final one.\n",
    "- **Shape:** `(batch_size, timesteps, units)` where `units` is the number of LSTM units.\n",
    "\n",
    "**When to use:**\n",
    "- You use `return_sequences=True` when you are stacking LSTM layers, and the next LSTM layer needs the full sequence of outputs, or when you're interested in predicting a sequence of values (e.g., sequence-to-sequence models).\n",
    "\n",
    "**Example Shape:**\n",
    "- Input shape: `(batch_size, timesteps, features)` (e.g., `(32, 10, 1)` for 10 timesteps with 1 feature each).\n",
    "- LSTM output shape: `(batch_size, timesteps, units)` (e.g., `(32, 10, 60)` if there are 60 LSTM units).\n",
    "\n",
    "### **Visual Example:**\n",
    "Let's say you have an input sequence of 10 timesteps, with 1 feature each (input shape `(batch_size, 10, 1)`), and you use an LSTM with 60 units.\n",
    "\n",
    "- **`return_sequences=False`:** Only the output from the last (10th) timestep is returned. The shape would be `(batch_size, 60)` because you're getting the output from only one timestep.\n",
    "  \n",
    "- **`return_sequences=True`:** The LSTM returns outputs for all 10 timesteps, so the shape is `(batch_size, 10, 60)`, with 60 units for each of the 10 timesteps.\n",
    "\n",
    "### **Usage in Stacked LSTM Layers:**\n",
    "- When stacking multiple LSTM layers, you need to set `return_sequences=True` for all LSTM layers except the last one so that each layer receives the full sequence of outputs from the previous one.\n",
    "- For example, the first LSTM layer outputs a sequence, which the second LSTM layer can then process.\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "# LSTM with return_sequences=True (returns output at each timestep)\n",
    "model.add(LSTM(units=60, return_sequences=True, input_shape=(10, 1)))\n",
    "\n",
    "# LSTM with return_sequences=False (returns output at the final timestep)\n",
    "model.add(LSTM(units=60, return_sequences=False))\n",
    "```\n",
    "\n",
    "### Summary:\n",
    "- **`return_sequences=True`:** Returns the full sequence of outputs (for each timestep).\n",
    "- **`return_sequences=False`:** Returns only the output at the final timestep.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
